---
title: "Single cell Project"
author: "Vittoria Ossanna"
output: html_notebook
editor_options: 
  markdown: 
    wrap: sentence
---

#### Load the necessary packages to perform the analysis

```{r, warning = F, message = F}

library(tidyverse) # collection of packages for tidy data analysis (ggplot, dplyr, ...)
library(Seurat) # single cell analysis (Check that the version is at least 4.0.0)
library(patchwork) # combine separate ggplots into the same graphic
```

### Reading the data

In this step, I am loading data from the file VPV.rds, a file containing an unknown sample obtained from either mice or human tissues.
After tissue dissociation, cells were sorted by FACS and scRNA-seq libraries were prepared with the Smart-Seq2 protocol/platform.
The starting point is a digital count matrix with mouse or human genes as features.

```{r, warning = F, message = F}
data.raw <- readRDS("./data/VPV.rds")
```

In this step, I am initializing the Seurat object with the digital count matrix.
We are starting with a count table containing 4148 samples and 23433 features (as genes).

```{r, warning = F, message = F}
data <- CreateSeuratObject(counts = data.raw)

# data
```

# 1. Quality control and filtering

In the current Section, I use standard methods of quality control and filtering to obtain a cleaned matrix of data.

## Filtering based on the percentage of mitocondrial, ribosomial and hemoglobin genes

We filter out genes with increased number of genes that map to a mitocondrial genome because low-quality or dying cells often exhibit extensive mitochondrial contamination.
In the same manner we will calculate the proportion gene expression that comes from ribosomal proteins.
Finally, with the same method we will calculate proportion hemoglobin genes, which can give an indication of red blood cell contamination.
I believe that this control has no use in this project because I do not know the composition of my sample, therefore, avoiding to consider hemoglobin-related genes could lead to errors.

As indicated in the assignment, mitochondrial genes might be missing from the count matrix, tehrefore I used spike-in RNAs, all starting with "ERCC", as an alternative quality control.
ERCC stands for External RNA Controls Consortium.

```{r qc_metrics, warning=F, message=F}
data[["percent_mt"]] <- PercentageFeatureSet(data, pattern = "^MT-")
data[["percent_ribo"]] <- PercentageFeatureSet(data, "^RP[SL]")
# data[["percent_hb"]] <- PercentageFeatureSet(data, "^HB[^(P)]")
max(data[["percent_mt"]])
max(data[["percent_ribo"]])
# max(data[["percent_hb"]])

data[["percent_ERCC"]] <- PercentageFeatureSet(data, "^ERCC")
# data[["percent_ERCC"]]
```

From this analysis, it seems that there is no instance that correspond to any of these genes.
Anyhow, the alternative filtering based on ERCC seems to produce non-null values.
I will use in the next steps this percentage for filtering.

### Updating dataset with ERCC filtering

This chunk is dedicate to the update of the data excluding all cells that present more than 5% of genes belonging ro ERCC.

Write to toma for the percentage

```{r}
data <- subset(data, percent_ERCC < 5)
```

## Filtering based on unique number of genes detected in each cell and total number of molecules detected within a cell

A standard approach is to filter cells with low amount of reads as well as genes that are present in at least a certain amount of cells.
Here we will only consider cells with at least 200 detected genes and genes need to be expressed in at least 3 cells.

```{r}
# selecting all features and cells with minimum requirement
selected_c <- WhichCells(data, expression = nFeature_RNA > 200)
selected_f <- rownames(data)[Matrix::rowSums(data) > 3]

data.filt <- subset(data, features = selected_f, cells = selected_c)
# dim(data.filt)
```

Extremely high number of detected genes could indicate doublets.
However, depending on the cell type composition in my sample, I may have cells with higher number of genes (and also higher counts) from one cell type.

I found these thresholds online [here](https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/seurat/seurat_01_qc.html#Calculate_QC) I need to write to toma

```{r}
#eliminate cells with an increased number of reads
high.data1 <- WhichCells(data.filt, expression = nFeature_RNA > 5000)

# remove these cells
final.data <- setdiff(WhichCells(data.filt),c(high.data1))

# check number of cells
# length(final.data)

# update the dataset with only genes that passed the filetring
data.filt <- data.filt[, colnames(data.filt) %in% final.data]

# check dimension of final filtered dataset
# dim(data.filt)

```

### Plot QC for ERCC genes

Now we can plot some of the QC-features to get visual insights.

```{r}
p1<- VlnPlot(data.filt, features = c("nFeature_RNA", "nCount_RNA", "percent_ERCC"), ncol = 3, pt.size = 0.01)

p1
```

```{r qc_scatter, warning=F, message=F}

plot1 <- FeatureScatter(data.filt, feature1 = "nCount_RNA", feature2 = "percent_ERCC")
plot2 <- FeatureScatter(data.filt, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2

```

## Filtering considerations

From the filtering based on presence of ERCC (excluding all cells that present more than 5% of ERCC transcripts detected) and the filtering based on a lower bound of counts (for both cells and genes) and upper bound (associated to Doublets), I recovered a dataset that contains 2284 cells and 22012 features.

# 2. Normalization, identification of variable features, scaling

## Data Normalization

In the realm of data analysis, ensuring proper normalization for both inter-sample and intra-sample comparisons is crucial to obtain unbiased results.
To achieve this goal in the current project, I employ a Log-Normalization method.
This approach normalizes the measurement of feature expression for each cell based on the total expression, applies a scaling factor, and subsequently transforms the results using a logarithmic function.
The resulting normalized data is stored within the `data.filt[["RNA"]]@data` container.

```{r, warning = F, message=F}

data.filt <- NormalizeData(data.filt, 
                           normalization.method = "LogNormalize", 
                           scale.factor = 10000)

# data.filt[["RNA"]]@data[1:10, 1:30]

```

## Feature Selection

In this section, I isolate a subset of features characterized by significant cell-to-cell variability.
Concentrating on these genes in subsequent analyses enhances the ability to emphasize biological signals within single-cell datasets.

```{r, message = F}
data.filt <- FindVariableFeatures(data.filt, 
                                  selection.method = "vst", 
                                  nfeatures = 2000)

top10 <- head(VariableFeatures(data.filt), 10)

plot1 <- VariableFeaturePlot(data.filt)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = T)

plot2

```

## Scaling

In this section, i perform a linear transformation to the expresison levels of each gene.
This is a standard pre-processing step which is often used before performing dimensionality reduction.
Since - at the end of the day - we are interested in the genes which are expressed in the most different way - we perform this scaling only on the top 2000 genes (identified before).

```{r, warning= F, message=F}
all_genes <- rownames(data.filt)

data.filt <- ScaleData(data.filt, 
                       fetaures = all_genes)

# data.filt[["RNA"]]@scale.data[1:5, 1:10]
```

# 3. Dimensionality reduction

Dimensionality reduction techniques are employed to reduce data complexity in downstream analyses (e.g. clustering) and for data visualization.

## PCA

PCA, or Principal Component Analysis, is a technique used to simplify complex data by transforming it into a lower-dimensional representation while retaining as much original information as possible.
It achieves this by finding a set of new variables called principal components that are uncorrelated and ranked by the amount of variance they explain.
PCA is useful for reducing data dimensionality, preserving essential information, and revealing underlying patterns.

```{r, message=F, warning=F}
data.filt <- RunPCA(data.filt, 
                    features = VariableFeatures(object = data.filt))

DimPlot(data.filt, reduction = "pca",label = F)
```

PCA is highly interpretable and efficient, nevertheless it is often not a good idea to use such dimentionality reduction method with Single Cell data, due to their high sparsity and non-linear stucture of the data.
For this reason, I use two different non-linear method in the next section to overcome this problem.

### Define number of relevant components

For next steps, I need to derive a proper number of dimensions to use.
In order to do so, I use the euristic method of the elbow plot: a ranking of principle components based on the percentage of variance explained by each one.

```{r}
ElbowPlot(data.filt, ndims = 30)
```

From this plot, we cannot clearly see an elbow, but we can see a slow decrease of the deviation from component 7 on.
Anyway, I consider a more conservative approach including in the next steps 14 dimensions.
Or maybe I can keep 7?
I REALLY DO NOT KNOW, I will try both and see what gives better results.

## t-SNE

t-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique used for visualizing complex high-dimensional data.
It works by calculating pairwise similarities between data points, emphasizing close similarities and using Gaussian distributions to define relationships.
It then maps the data to a lower-dimensional space (typically two dimensions) while preserving these pairwise similarities.
t-SNE employs an iterative optimization process to position data points in the lower-dimensional map, ensuring that similar data points remain close, while dissimilar ones are separated.
This makes it a valuable tool for exploring and visualizing intricate data structures and identifying clusters or patterns within the data.

```{r}
data.filt <- RunTSNE(data.filt, 
                     dims = 1:7)
DimPlot(data.filt, reduction = "tsne")
```

## UMAP

UMAP (Uniform Manifold Approximation and Projection) is another dimensionality reduction technique used for visualizing and exploring high-dimensional data.
It shares similarities with t-SNE but offers certain advantages.
UMAP constructs a low-dimensional representation by first modeling the data as a topological graph.
It focuses on preserving both local and global relationships among data points, leading to more balanced embeddings.
UMAP is known for its efficiency and ability to handle larger datasets compared to t-SNE.
It has gained popularity in various fields for tasks such as visualization, clustering, and data analysis, making it a valuable tool for uncovering patterns and structures within complex datasets.
From 2018, UMAP is considered the gold standard for Single Cell analysis.
In addition, it preserves better than t-SNE the local and global structure of the data.
Nevertheless, UMAP results decrease their interpretability.

```{r, message=F, warning=F}
data.filt <- RunUMAP(data.filt,
                     dims = 1:7)

DimPlot(data.filt, reduction = "umap")
```

# 4. Clustering

# 5. Identification of marker genes

# 6. Cell cycle analysis

# 7. Annotation
